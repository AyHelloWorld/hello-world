
假设集群维护一个单个索引并有一个分片和一个复制节点。节点1在启动时被选举为主节点并保存主分片，而节点2保存复制分片；
如果在两个节点之间的通讯中断了，两个节点都相信对方已经挂了。节点1不需要做什么，因为它本来就被选举为主节点。但是节点2会自动选举它自己为主节点，因为它相信集群的一部分没有主节点了。
在elasticsearch集群，是有主节点来决定将分片平均的分布到节点上的。节点2保存的是复制分片，但它相信主节点不可用了。所以它会自动提升复制节点为主节点。
现在集群在一个不一致的状态了。打在节点1上的索引请求会将索引数据分配在主节点，同时打在节点2的请求会将索引数据放在分片上。在这种情况下，分片的两份数据分开了，
如果不做一个全量的重索引很难对它们进行重排序。在更坏的情况下，一个对集群无感知的索引客户端（例如，使用REST接口的）,这个问题非常透明难以发现，
无论哪个节点被命中索引请求仍然在每次都会成功完成。问题只有在搜索数据时才会被隐约发现：取决于搜索请求命中了哪个节点，结果都会不同。

要预防脑裂问题，我们需要看的一个参数就是 discovery.zen.minimum_master_nodes。这个参数决定了在选主过程中需要 有多少个节点通信。
缺省配置是1.一个基本的原则是这里需要设置成 N/2+1, N是急群中节点的数量。 例如在一个三节点的集群中， minimum_master_nodes应该被设为 3/2 + 1 = 2(四舍五入)。

让我们想象下之前的情况下如果我们把 discovery.zen.minimum_master_nodes 设置成 2（2/2 + 1）。当两个节点的通信失败了，节点1会失去它的主状态，同时节点2也不会被选举为主。
没有一个节点会接受索引或搜索的请求，让所有的客户端马上发现这个问题。而且没有一个分片会处于不一致的状态。

我们可以调的另一个参数是 discovery.zen.ping.timeout。它的默认值是3秒并且它用来决定一个节点在假设集群中的另一个节点响应失败的情况时等待多久。
在一个慢速网络中将这个值调的大一点是个不错的主意。这个参数不止适用于高网络延迟，还能在一个节点超载响应很慢时起作用。

建议配置一个3节点集群。这样你可以设置minimum_master_nodes为2，减少了脑裂的可能性，但仍然保持了高可用的优点：你可以承受一个节点失效但集群还是正常运行的。

可以设置 node.data 参数来选择这个节点是否需要保存数据。缺省值是“true”，意思是默认每个elasticsearch节点同时也会作为一个数据节点。

可以添加一个新节点并把 node.data 参数设置为“false”。这样这个节点不会保存任何分片，但它仍然可以被选为主（默认行为）。

主节点
在192.168.3.145主机上：
gswewf@gswewf-PC:~/docker/elasticsearch$ mkdir config_master
gswewf@gswewf-PC:~/docker/elasticsearch$ mkdir master_data
gswewf@gswewf-PC:~/docker/elasticsearch$ vim config_master/elasticsearch.yml

cluster.name: elasticsearch_cluster
node.name: master
node.master: true
node.data: false
network.host: 0.0.0.0
network.publish_host: 192.168.3.145

# 设置节点之间交互的tcp端口，默认是9300。
transport.tcp.port: 9300

# 设置对外服务的http端口，默认为9200。
http.port: 9200

http.cors.enabled: true
http.cors.allow-origin: "*"

# 这个设置为了避免脑裂。比如3个节点的集群，如果设置为2，那么当一台节点脱离后，不会自动成为master。
# discovery.zen.minimum_master_nodes: 2


# 设置ping其他节点时的超时时间，网络比较慢时可将该值设大：
# 是设置了节点与节点之间的连接ping时长
# discovery.zen.ping_timeout: 120s

discovery.zen.ping.unicast.hosts: ["192.168.3.145:9300", "192.168.3.145:9301", "192.168.3.105:9301"]
# discovery.zen.ping.unicast.hosts: ["192.168.3.145:9300"]


gswewf@gswewf-PC:~/docker/elasticsearch$ docker run -d --name elasticsearch_cluster_master_5.6.4_9200 -p 9200:9200 -p 9300:9300 -v "$PWD/master_data":/usr/share/elasticsearch/data -v $PWD/config_master/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /etc/localtime:/etc/localtime -v $PWD/plugins_5.6.4/elasticsearch-analysis-ik-5.6.4:/usr/share/elasticsearch/plugins/elasticsearch-analysis-ik-5.6.4 -v $PWD/plugins_5.6.4/elasticsearch-analysis-pinyin-5.6.4:/usr/share/elasticsearch/plugins/elasticsearch-analysis-pinyin-5.6.4 elasticsearch:5.6.4
f379af5c5d0e8e6ec1d99e8908bde45943d36e777ef17d6816d89382527a4610

添加数据节点1
在192.168.3.145主机上：
gswewf@gswewf-PC:~/docker/elasticsearch$ mkdir config_node_1
gswewf@gswewf-PC:~/docker/elasticsearch$ mkdir slave1_data
gswewf@gswewf-PC:~/docker/elasticsearch$ vim config_node_1/elasticsearch.yml

cluster.name: elasticsearch_cluster
node.name: slave1
node.master: false
node.data: true
network.host: 0.0.0.0
network.publish_host: 192.168.3.145
# 设置节点之间交互的tcp端口，默认是9300。
transport.tcp.port: 9301

# 设置对外服务的http端口，默认为9200。
http.port: 9201

http.cors.enabled: true
http.cors.allow-origin: "*"

# 这个设置为了避免脑裂。比如3个节点的集群，如果设置为2，那么当一台节点脱离后，不会自动成为master。
# discovery.zen.minimum_master_nodes: 2

# 设置ping其他节点时的超时时间，网络比较慢时可将该值设大：
# 是设置了节点与节点之间的连接ping时长
# discovery.zen.ping_timeout: 120s

discovery.zen.ping.unicast.hosts: ["192.168.3.145:9300", "192.168.3.145:9301", "192.168.3.105:9301"]
# discovery.zen.ping.unicast.hosts: ["192.168.3.145:9300"]

gswewf@gswewf-PC:~/docker/elasticsearch$ docker run -d --name elasticsearch_cluster_node_1_5.6.4_9201 -p 9201:9201 -p 9301:9301 -v "$PWD/slave1_data":/usr/share/elasticsearch/data -v $PWD/config_node_1/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /etc/localtime:/etc/localtime -v $PWD/plugins_5.6.4/elasticsearch-analysis-ik-5.6.4:/usr/share/elasticsearch/plugins/elasticsearch-analysis-ik-5.6.4 -v $PWD/plugins_5.6.4/elasticsearch-analysis-pinyin-5.6.4:/usr/share/elasticsearch/plugins/elasticsearch-analysis-pinyin-5.6.4 elasticsearch:5.6.4
f218c94f3ebbd60151995bde7315d57744fbcfb8c8052c409968b38102318215


添加数据节点2
在192.168.3.105机器上：
zy@ubuntu:~/docker/elasticsearch$ mkdir config_node_2
zy@ubuntu:~/docker/elasticsearch$ mkdir slave2_data
zy@ubuntu:~/docker/elasticsearch$ vim config_node_2/elasticsearch.yml

cluster.name: elasticsearch_cluster
node.name: slave2
node.master: false
node.data: true
network.host: 0.0.0.0
network.publish_host: 192.168.3.105
# 设置节点之间交互的tcp端口，默认是9300。
transport.tcp.port: 9301

# 设置对外服务的http端口，默认为9200。
http.port: 9201

http.cors.enabled: true
http.cors.allow-origin: "*"

# 这个设置为了避免脑裂。比如3个节点的集群，如果设置为2，那么当一台节点脱离后，不会自动成为master。
# discovery.zen.minimum_master_nodes: 2


# 设置ping其他节点时的超时时间，网络比较慢时可将该值设大：
# 是设置了节点与节点之间的连接ping时长
# discovery.zen.ping_timeout: 120s

discovery.zen.ping.unicast.hosts: ["192.168.3.145:9300", "192.168.3.145:9301", "192.168.3.105:9301"]
# discovery.zen.ping.unicast.hosts: ["192.168.3.145:9300"]

zy@ubuntu:~/docker/elasticsearch$ docker run -d --name elasticsearch_cluster_node_2_5.6.4_9201 -p 9201:9201 -p 9301:9301 -v "$PWD/slave2_data":/usr/share/elasticsearch/data -v $PWD/config_node_2/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /etc/localtime:/etc/localtime -v $PWD/plugins_5.6.4/elasticsearch-analysis-ik-5.6.4:/usr/share/elasticsearch/plugins/elasticsearch-analysis-ik-5.6.4 -v $PWD/plugins_5.6.4/elasticsearch-analysis-pinyin-5.6.4:/usr/share/elasticsearch/plugins/elasticsearch-analysis-pinyin-5.6.4 elasticsearch:5.6.4
4934b34ba3baac621acc0e95e7c22131d75266894eeda732e8d52e9e221edf7f

更新集群主节点的数量
curl -XPUT 'localhost:9200/_cluster/settings?flat_settings=true&pretty' -H 'Content-Type: application/json' -d'
 {
     "transient" : {
         "discovery.zen.minimum_master_nodes" : 2
     }
 }
 '

默认生成的索引是有一份副本文件的
这种情况虽然健康值为绿色，但是当中两个节点宕机的极端情况下，会出现数据丢失的风险。
可以通过elasticsearch-kopf插件进行修改副本分片数达到所有节点分片数一致。
修改number_of_replicas
修改后
如果想自定义导入数据时的分片数量，可以在logstash目录下新建/config/logstash-simple.conf文件，对output到es的数据格式进行自定义。通过新建json配置文件来对索引格式自定义。
"number_of_replicas":"4",  对应副本分片数（可更改）
"number_of_shards":"4" 对应主分片数（索引创建就无法更改）

https://www.jianshu.com/p/a164b86445b6?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation

今天发现logstash没有任何数据写入elasticsearch，检查logs，发现报错high disk watermark [10%] exceeded on [asfasdf112xzvdx][flag=smasher] free:2.8gb[9.2%], shards will be relocated away from this node.

原因可能是机器剩余磁盘空间不足，低于elasticsearch的shard设置，没办法了删除部分没用的数据。

更新2015-12-16

可以在shell用命令关闭这个检查

curl -XPUT localhost:9200/_cluster/settings -d '{

    "transient" : {

        "cluster.routing.allocation.disk.threshold_enabled" : false

    }

}'

Elasticsearch使用两个配置参数决定分片是否能存放到某个节点上。cluster.routing.allocation.disk.watermark.low：控制磁盘使用的低水位。
默认为85%，意味着如果节点磁盘使用超过85%，则ES不允许在分配新的分片。当配置具体的大小如100MB时，表示如果磁盘空间小于100MB不允许分配分片。
cluster.routing.allocation.disk.watermark.high：控制磁盘使用的高水位。默认为90%，意味着如果磁盘空间使用高于90%时，ES将尝试分配分片到其他节点。
上述两个配置可以使用API动态更新，ES每隔30s获取一次磁盘的使用信息，该值可以通过cluster.info.update.interval来设置。

当多主机分布式集群时候，若某台主机硬盘不足的情况下，可能导致对应的es数据写入不进去；导致查看集群状态的时候，是yellow，该主机没有分片数据；

